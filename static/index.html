<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>A Forecasting Study on Swedish Wine Sales</title>
    <meta charset="utf-8" />
    <meta name="author" content="Jonathan Berrisch &amp; Timo Rammert" />
    <link href="index_files/remark-css-0.0.1/default.css" rel="stylesheet" />
    <script src="index_files/kePrint-0.0.1/kePrint.js"></script>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        TeX: { equationNumbers: { autoNumber: "AMS" } },
      });
    </script>
    <style>
    .mjx-mrow a {
      color: black;
      pointer-events: none;
      cursor: default;
    }
    </style>
    <link rel="stylesheet" href="assets\sydney-fonts.css" type="text/css" />
    <link rel="stylesheet" href="assets\sydney.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# A Forecasting Study on Swedish Wine Sales <svg style="height:0.8em;top:.04em;position:relative;fill:#b50000;" viewBox="0 0 512 512"><path d="M507.31 72.57L439.43 4.69c-6.25-6.25-16.38-6.25-22.63 0l-22.63 22.63c-6.25 6.25-6.25 16.38 0 22.63l-76.67 76.67c-46.58-19.7-102.4-10.73-140.37 27.23L18.75 312.23c-24.99 24.99-24.99 65.52 0 90.51l90.51 90.51c24.99 24.99 65.52 24.99 90.51 0l158.39-158.39c37.96-37.96 46.93-93.79 27.23-140.37l76.67-76.67c6.25 6.25 16.38 6.25 22.63 0l22.63-22.63c6.24-6.24 6.24-16.37-.01-22.62zM179.22 423.29l-90.51-90.51 122.04-122.04 90.51 90.51-122.04 122.04z"/></svg>
## Term Paper Statistical Learning
### Jonathan Berrisch &amp; Timo Rammert
### 23.09.2019

---

background-image: url("assets/ml_plot.png")
background-position: 100% 50%
background-size: 50% 75%

# Agenda

&lt;/br&gt;

.pull-left[
.font130[
.content-box-blue[
1. Introduction
2. Data and Variables
3. Validation Approach
4. Analysis
5. Robustness
6. Final Model
7. Discussion
]
]
]

---



class: segue, center, middle

# Introduction

# &lt;svg style="height:0.8em;top:.04em;position:relative;fill:white;" viewBox="0 0 640 512"&gt;&lt;path d="M208 352c-2.39 0-4.78.35-7.06 1.09C187.98 357.3 174.35 360 160 360c-14.35 0-27.98-2.7-40.95-6.91-2.28-.74-4.66-1.09-7.05-1.09C49.94 352-.33 402.48 0 464.62.14 490.88 21.73 512 48 512h224c26.27 0 47.86-21.12 48-47.38.33-62.14-49.94-112.62-112-112.62zm-48-32c53.02 0 96-42.98 96-96s-42.98-96-96-96-96 42.98-96 96 42.98 96 96 96zM592 0H208c-26.47 0-48 22.25-48 49.59V96c23.42 0 45.1 6.78 64 17.8V64h352v288h-64v-64H384v64h-76.24c19.1 16.69 33.12 38.73 39.69 64H592c26.47 0 48-22.25 48-49.59V49.59C640 22.25 618.47 0 592 0z"/&gt;&lt;/svg&gt;



---

# Introduction

&lt;/br&gt;

.pull-left[

.content-box-blue[

Goal:
  - Forecasting Swedish wine sales in litres
  - Identification of determinants of wine sales

Basis:
  - Dataset by Friberg and Grönqvist (2012)
  - Methods and procedures based on ‘An Introduction to Statistical Learning with Applications in R’
]]


.pull-right[

.content-box-blue[

Predictions based on existing Literature:

- Expert opinion increases sales - depending on received reviews (Hilger et al. (2011), Ashenfelter and Jones (2013))
- Regional influences for local wine sales (Bicknell and MacDonald (2012))

]]

&lt;/br&gt;

???

Dataset previously analyzed in Article in American Economic Journal

---
class: inverse, center, middle 

# Data and Variables

&lt;/br&gt;

&lt;svg style="height:1.8em;top:.04em;position:relative;fill:white;" viewBox="0 0 512 512"&gt;&lt;path d="M464 32H48C21.49 32 0 53.49 0 80v352c0 26.51 21.49 48 48 48h416c26.51 0 48-21.49 48-48V80c0-26.51-21.49-48-48-48zM160 448H48c-8.837 0-16-7.163-16-16v-80h128v96zm0-128H32v-96h128v96zm0-128H32V96h128v96zm160 256H192v-96h128v96zm0-128H192v-96h128v96zm0-128H192V96h128v96zm160 160v80c0 8.837-7.163 16-16 16H352v-96h128zm0-32H352v-96h128v96zm0-128H352V96h128v96z"/&gt;&lt;/svg&gt;

---

# Data and Variables

## Overview

.pull-left[

.content-box-blue[

Data set train:
  - Sales from 2002-01-02 until 2006-02-22
  - 145179 observations of 58 variables (before preprocessing)
  - 41416 observations of 45 variables (after preprocessing)
]]

.pull-right[

.content-box-blue[


Data set test:
  - Sales from 2006-02-22 until 2007-01-10
  - 48395 observations of 58 variables (before preprocessing)
  - 9783 observations of 45 variables (after preprocessing)
]]

&lt;/br&gt;

&lt;table&gt;
&lt;caption&gt;Frequency of Variable Types.&lt;/caption&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:right;"&gt; Date &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; factor &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; logical &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; numeric &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 8 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 19 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 29 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

???

- Densities of variable litre is nearly the same for train (from 0 to 184200) and test (from 0 to 184398)
- Variables: name of wine, country, price, amount sold per week,market share, segments, reviews-related variables, etc.
- No great differences in prices (only in terms of max in train): 
  - price in train: from 38 to 1149 with mean 108 and median 79
  - price in test: from 43 to 349 with mean 108 and median 87
- Difference in segments: test only includes red, sparkling and white

---

# Data and Variables

## Missing Values

<iframe src="p.missings.html" width="100%" height="525px" scrolling="no" seamless="seamless" frameBorder="0"></iframe>

---
class: inverse, center, middle

# Validation Approach

# &lt;svg style="height:0.8em;top:.04em;position:relative;fill:white;" viewBox="0 0 512 512"&gt;&lt;path d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/&gt;&lt;/svg&gt; 

---

# Validation Approach

.pull-left[

.content-box-blue[

### Theoretical

- Crossvalidation using 5 folds


- RMSE `$$\sqrt{\frac{1}{n}\sum_{i = 1}^{n}\left(y_i-\hat{y}_i\right)^2}$$` as key performance measure for evaluating models


- Further robustness checks for assesing validity of results obtained

]
]

.pull-right[

.content-box-blue[

### Technical

- Using Rstudio Jobs Feature
  - Multicore Processing
  - Model independent CV approach
  
```{}
library(rstudioapi)

for(i in 1:5){

  fold_data &lt;- data_list[[i]]
  jobRunScript("path_to_model",
               workingDir = "../statl",
               importEnv = T)
               
}
```

]
]

???

- 5 Folds because trade-off between validation and computational effort


---

class: inverse, center, middle

# Analysis

&lt;/br&gt;

&lt;svg style="height:40;fill:white;" viewBox="0 0 640 512"&gt;&lt;path d="M257.981 272.971L63.638 467.314c-9.373 9.373-24.569 9.373-33.941 0L7.029 444.647c-9.357-9.357-9.375-24.522-.04-33.901L161.011 256 6.99 101.255c-9.335-9.379-9.317-24.544.04-33.901l22.667-22.667c9.373-9.373 24.569-9.373 33.941 0L257.981 239.03c9.373 9.372 9.373 24.568 0 33.941zM640 456v-32c0-13.255-10.745-24-24-24H312c-13.255 0-24 10.745-24 24v32c0 13.255 10.745 24 24 24h304c13.255 0 24-10.745 24-24z"/&gt;&lt;/svg&gt; 
  
---
# Analysis

## Mean Regression and Linear Regression

&lt;br&gt;

Baseline methods for comparing the predictions made by further methods

&lt;br&gt;

.pull-left[

  .content-box-blue[

Mean Regression:
- Mean litres sold: 6886
- RMSE: 13340


  ]

]

.pull-right[

  .content-box-blue[

Linear Regression:
- Factor variables transformed into dummy variables `\(\rightarrow\)` 757 variables
- RMSE: 5572


  ]
]

---

# Analysis

## Least Absolute Shrinkage and Selection Operator
&lt;br&gt;

.font130[
.content-box-blue[
- Method combining linear regression and shrinking of coefficient estimates
- Fits linear model constrained by penalty term, i.e., it minimizes `\(\sum_{i=1}^{n}(y_i - \beta_0 - \sum_{j=1}^{p}\beta_jx_{ij})^2+\lambda\sum_{j=1}^{p}|\beta_j|.\)`
- RMSE: 5548
]
]

???

Also calculated Lasso for loglitres: RMSE of 5812

---

# Analysis

## Least Absolute Shrinkage and Selection Operator

&lt;div style="position:relative; margin-top:-25px; z-index: 0"&gt;

<iframe src="p.lasso.html" width="100%" height="525px" scrolling="no" seamless="seamless" frameBorder="0"></iframe>

&lt;/div&gt;

---

# Analysis

## Principal Components Regression and Partial Least Squares

&lt;br&gt;

.pull-left[

.content-box-blue[

### Principal Components Regression

- Unsupervised transformation of X
- May reduce the dimension
- Results are hard to interpret
- Number of covariates (by CV): `\(\approx 635\)`
- RMSE: `\(\approx 5546.05\)`

]
]

.pull-right[

.content-box-blue[

### Partial Least Squares

- Supervised Transformation of X
- Highest weight on covariates that are strongly related to Y
- Number of covariates (by CV): `\(\approx 35\)`
- RMSE: `\(\approx 6201.15\)`

]
]

---

# Analysis

## Principal Components Regression and Partial Least Squares

&lt;div style="position:relative; margin-top:-25px; z-index: 0"&gt;

<iframe src="p.pca.html" width="100%" height="525px" scrolling="no" seamless="seamless" frameBorder="0"></iframe>

&lt;/div&gt;

---

# Analysis

## Splines

&lt;br&gt;

.pull-left[

.content-box-blue[

### Overview

- Piecewise polynomial regression
- Splines for 'year' 'price' and 'rprice'
- Up to 20 Knots
- Knots are placed at suitable quantiles

]
]

.pull-right[

.content-box-blue[

### Results

- 0  Knots: RMSE `\(5572\)`
  - This equals the Linear Model
- 20 Knots: RMSE `\(5507\)`
  - Slight improvement 

]
]

---

# Analysis

## Splines

<iframe src="p.splines.html" width="97%" height="500" scrolling="no" seamless="seamless" frameBorder="0"></iframe>


---

class: segue, center, middle

# Tree Based Methods

&lt;br&gt;

&lt;svg style="height:40;top:.04em;position:relative;fill:white;" viewBox="0 0 640 512"&gt;&lt;path d="M634.19 376.23l-55.47-64.37H584c9.34 0 17.84-5.43 21.78-13.92 3.94-8.47 2.56-18.48-3.47-25.61l-54.56-64.55H560c9.47 0 18.06-5.58 21.94-14.24a24.088 24.088 0 0 0-4.09-25.85l-144-160.11c-9.13-10.1-26.56-10.1-35.69 0L320 94.48l-78.16-86.9c-9.13-10.1-26.56-10.1-35.69 0l-144 160.11a24.063 24.063 0 0 0-4.09 25.85c3.88 8.66 12.47 14.24 21.94 14.24h12.25l-54.56 64.55c-6.03 7.13-7.41 17.14-3.47 25.61A24.021 24.021 0 0 0 56 311.86h5.28L5.81 376.23c-6.13 7.11-7.53 17.15-3.63 25.69a23.998 23.998 0 0 0 21.81 14.01h168v24.46l-30.29 48.43c-5.32 10.65 2.42 23.17 14.31 23.17h95.96c11.89 0 19.63-12.53 14.31-23.17L256 440.39v-24.46h128v24.46l-30.29 48.43c-5.32 10.65 2.42 23.17 14.31 23.17h95.96c11.89 0 19.63-12.53 14.31-23.17L448 440.39v-24.46h168c9.38 0 17.91-5.47 21.81-14.01 3.91-8.54 2.51-18.57-3.62-25.69zM304 367.9H76.37l89.66-104.07h-58.28l88-104.07h-61.88L224 59.55l90.13 100.2h-61.88l88 104.07h-58.28l89.66 104.07H304zm131 0l-48.29-56.04H392c9.34 0 17.84-5.43 21.78-13.92 3.94-8.47 2.56-18.48-3.47-25.61l-54.56-64.55H368c9.47 0 18.06-5.58 21.94-14.24a24.088 24.088 0 0 0-4.09-25.85l-33.55-37.31L416 59.55l90.13 100.2h-61.88l88 104.07h-58.28l89.66 104.07H435z"/&gt;&lt;/svg&gt;

---

background-image: url("assets/tree.png")
background-position: 100% 100%
background-size: 50% 75%

# Analysis

## Single Regression Tree

.pull-left[

&lt;br&gt;

.content-box-blue[

- A tree is grown by splitting the feature space until terminal nodes reach a set minimum size
- Algorithm decides automatically on splitting variables and points 
- Minimizing sum of squared residuals
- Same variables chosen for splitting for every fold
- RMSE: 6495

]
]

---

background-image: url("assets/tree_pruned.png")
background-position: 100% 100%
background-size: 50% 75%

# Analysis

## Pruned Tree

.pull-left[

&lt;br&gt;

.font120[
.content-box-blue[

- Selecting subtree with lowest test error via validation approach

 `\(\rightarrow\)` cost complexity pruning
- RMSE: 7936

]
]
]

---

# Analysis

## Bootstrap Aggregation

&lt;br&gt;

.content-box-blue[
.font120[

- Repeated drawing of bootstrap samples before growing trees
- Averaging of results afterwards
- Lowers variance compared to single regression trees
- Tuning parameter: Number of trees
- RMSE: 4306 (25 Trees)

]
]

---

# Analysis

## Bootstrap Aggregation

&lt;div style="position:relative; margin-top:-50px; z-index: 0"&gt;

<iframe src="p.bagging.html" width="97%" height="500" scrolling="no" seamless="seamless" frameBorder="0"></iframe>
&lt;/div&gt;
---

# Analysis

## Random Forest

&lt;br&gt;

.content-box-blue[
.font120[
- Special case of bagging: only pre-specified number of randomly chosen variables consindered for splitting

 `\(\rightarrow\)` decorrelates trees
- Variation over number of variables considered at each split and number of trees grown
]
]

---

# Analysis

## Random Forest

&lt;div style="position:relative; margin-top:-150px; z-index: 0"&gt;

<iframe src="p.rf.html" width="97%" height="650" scrolling="no" seamless="seamless" frameBorder="0"></iframe>

&lt;/div&gt;

---

# Analysis

## Random Forest

### Variable Importance

&lt;div style="position:relative; margin-top:-50px; z-index: 0"&gt;

<iframe src="p.rf_varimp.html" width="97%" height="500" scrolling="no" seamless="seamless" frameBorder="0"></iframe>

&lt;/div&gt;

---
# Analysis

## Random Forest

### Partial Dependence
&lt;img src="index_files/figure-html/unnamed-chunk-9-1.png" width="1080" /&gt;


---

# Analysis

## Boosting

&lt;br&gt;

.content-box-blue[
.font110[

- Sequential growing of trees using the resiudals of previous grown trees
 
 `\(\rightarrow\)` Newly grown trees depend on earlier ones 
- Tuning parameters: Number of trees, interaction depth, shrinkage parameter `\(\lambda\)`
- Best boosting model: 25 trees, interaction depth of 15, `\(\lambda =0.4\)`
- RMSE-Test: 4479
]
]

---

# Analysis

## Boosting

&lt;div style="position:relative; margin-top:-150px; z-index: 0"&gt;

<iframe src="p.boosting.html" width="97%" height="650" scrolling="no" seamless="seamless" frameBorder="0"></iframe>

&lt;/div&gt;

---

class: inverse, center, middle

# Robustness

# &lt;svg style="height:0.8em;top:.04em;position:relative;fill:white;" viewBox="0 0 576 512"&gt;&lt;path d="M571.31 193.94l-22.63-22.63c-6.25-6.25-16.38-6.25-22.63 0l-11.31 11.31-28.9-28.9c5.63-21.31.36-44.9-16.35-61.61l-45.25-45.25c-62.48-62.48-163.79-62.48-226.28 0l90.51 45.25v18.75c0 16.97 6.74 33.25 18.75 45.25l49.14 49.14c16.71 16.71 40.3 21.98 61.61 16.35l28.9 28.9-11.31 11.31c-6.25 6.25-6.25 16.38 0 22.63l22.63 22.63c6.25 6.25 16.38 6.25 22.63 0l90.51-90.51c6.23-6.24 6.23-16.37-.02-22.62zm-286.72-15.2c-3.7-3.7-6.84-7.79-9.85-11.95L19.64 404.96c-25.57 23.88-26.26 64.19-1.53 88.93s65.05 24.05 88.93-1.53l238.13-255.07c-3.96-2.91-7.9-5.87-11.44-9.41l-49.14-49.14z"/&gt;&lt;/svg&gt;

---

# Robustness

&lt;br&gt;

.content-box-blue[

Calculation of various alternative estimations, especially:

- Inclusion of variables “time_segm_price” and “artikpr”
  - Have been excluded beforehand as they are combinations of existing variables
  - Including these yields a higher RMSE -&gt; No improvement of the model
  
- Reducing share of NAs for removing variables from the dataset
  - Beforehand: 50%
  - Reducing it to 20%
  - Gain in No. of variables and observations does not lead to higher precision

]
---

class: inverse, center, middle

# Conclusion and Evaluation

# &lt;svg style="height:0.8em;top:.04em;position:relative;fill:white;" viewBox="0 0 512 512"&gt;&lt;path d="M243.2 189.9V258c26.1 5.9 49.3 15.6 73.6 22.3v-68.2c-26-5.8-49.4-15.5-73.6-22.2zm223.3-123c-34.3 15.9-76.5 31.9-117 31.9C296 98.8 251.7 64 184.3 64c-25 0-47.3 4.4-68 12 2.8-7.3 4.1-15.2 3.6-23.6C118.1 24 94.8 1.2 66.3 0 34.3-1.3 8 24.3 8 56c0 19 9.5 35.8 24 45.9V488c0 13.3 10.7 24 24 24h16c13.3 0 24-10.7 24-24v-94.4c28.3-12.1 63.6-22.1 114.4-22.1 53.6 0 97.8 34.8 165.2 34.8 48.2 0 86.7-16.3 122.5-40.9 8.7-6 13.8-15.8 13.8-26.4V95.9c.1-23.3-24.2-38.8-45.4-29zM169.6 325.5c-25.8 2.7-50 8.2-73.6 16.6v-70.5c26.2-9.3 47.5-15 73.6-17.4zM464 191c-23.6 9.8-46.3 19.5-73.6 23.9V286c24.8-3.4 51.4-11.8 73.6-26v70.5c-25.1 16.1-48.5 24.7-73.6 27.1V286c-27 3.7-47.9 1.5-73.6-5.6v67.4c-23.9-7.4-47.3-16.7-73.6-21.3V258c-19.7-4.4-40.8-6.8-73.6-3.8v-70c-22.4 3.1-44.6 10.2-73.6 20.9v-70.5c33.2-12.2 50.1-19.8 73.6-22v71.6c27-3.7 48.4-1.3 73.6 5.7v-67.4c23.7 7.4 47.2 16.7 73.6 21.3v68.4c23.7 5.3 47.6 6.9 73.6 2.7V143c27-4.8 52.3-13.6 73.6-22.5z"/&gt;&lt;/svg&gt;

---

# Conclusion and Evaluation

.pull-left[

.content-box-blue[

.center[.font130[Conclusion]]

&lt;table class="table table-hover" style="margin-left: auto; margin-right: auto;"&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; Model &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; Mean &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Mean Regression &lt;/td&gt;
   &lt;td style="text-align:left;width: 5cm; "&gt; &lt;span style="display: inline-block; direction: rtl; border-radius: 4px; padding-right: 2px; background-color: #c8ffc2; width: 100.00%"&gt;13340&lt;/span&gt; &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Linear Regression &lt;/td&gt;
   &lt;td style="text-align:left;width: 5cm; "&gt; &lt;span style="display: inline-block; direction: rtl; border-radius: 4px; padding-right: 2px; background-color: #c8ffc2; width: 41.77%"&gt;5572&lt;/span&gt; &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Lasso &lt;/td&gt;
   &lt;td style="text-align:left;width: 5cm; "&gt; &lt;span style="display: inline-block; direction: rtl; border-radius: 4px; padding-right: 2px; background-color: #c8ffc2; width: 41.59%"&gt;5548&lt;/span&gt; &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; PCR &lt;/td&gt;
   &lt;td style="text-align:left;width: 5cm; "&gt; &lt;span style="display: inline-block; direction: rtl; border-radius: 4px; padding-right: 2px; background-color: #c8ffc2; width: 41.57%"&gt;5546&lt;/span&gt; &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; PLS &lt;/td&gt;
   &lt;td style="text-align:left;width: 5cm; "&gt; &lt;span style="display: inline-block; direction: rtl; border-radius: 4px; padding-right: 2px; background-color: #c8ffc2; width: 46.48%"&gt;6201&lt;/span&gt; &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Splines (20 Knots) &lt;/td&gt;
   &lt;td style="text-align:left;width: 5cm; "&gt; &lt;span style="display: inline-block; direction: rtl; border-radius: 4px; padding-right: 2px; background-color: #c8ffc2; width: 41.28%"&gt;5507&lt;/span&gt; &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Single Tree &lt;/td&gt;
   &lt;td style="text-align:left;width: 5cm; "&gt; &lt;span style="display: inline-block; direction: rtl; border-radius: 4px; padding-right: 2px; background-color: #c8ffc2; width: 48.69%"&gt;6495&lt;/span&gt; &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Bagging (25 Trees) &lt;/td&gt;
   &lt;td style="text-align:left;width: 5cm; "&gt; &lt;span style="display: inline-block; direction: rtl; border-radius: 4px; padding-right: 2px; background-color: #c8ffc2; width: 32.28%"&gt;4306&lt;/span&gt; &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Random Forest (Best) &lt;/td&gt;
   &lt;td style="text-align:left;width: 5cm; "&gt; &lt;span style="display: inline-block; direction: rtl; border-radius: 4px; padding-right: 2px; background-color: #c8ffc2; width: 32.79%"&gt;4374&lt;/span&gt; &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Boosting (Best) &lt;/td&gt;
   &lt;td style="text-align:left;width: 5cm; "&gt; &lt;span style="display: inline-block; direction: rtl; border-radius: 4px; padding-right: 2px; background-color: #c8ffc2; width: 33.58%"&gt;4479&lt;/span&gt; &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;br&gt;
]
]
.pull-right[

.content-box-blue[

.center[.font130[Evaluation]]

- Random Forest as prefered model
  - Substantially lower computation time than Bagging
  - Slightly lower RMSE
- Performance on Test Dataset: `\(6738\)`
  - `\(6546\)` if we ommit one outlier
  - Re-Estimation of the final model was nessecary due to missing features in test set

]
]

---

class: inverse, center, middle

# Discussion

# &lt;svg style="height:0.8em;top:.04em;position:relative;fill:white;" viewBox="0 0 576 512"&gt;&lt;path d="M416 192c0-88.4-93.1-160-208-160S0 103.6 0 192c0 34.3 14.1 65.9 38 92-13.4 30.2-35.5 54.2-35.8 54.5-2.2 2.3-2.8 5.7-1.5 8.7S4.8 352 8 352c36.6 0 66.9-12.3 88.7-25 32.2 15.7 70.3 25 111.3 25 114.9 0 208-71.6 208-160zm122 220c23.9-26 38-57.7 38-92 0-66.9-53.5-124.2-129.3-148.1.9 6.6 1.3 13.3 1.3 20.1 0 105.9-107.7 192-240 192-10.8 0-21.3-.8-31.7-1.9C207.8 439.6 281.8 480 368 480c41 0 79.1-9.2 111.3-25 21.8 12.7 52.1 25 88.7 25 3.2 0 6.1-1.9 7.3-4.8 1.3-2.9.7-6.3-1.5-8.7-.3-.3-22.4-24.2-35.8-54.5z"/&gt;&lt;/svg&gt;
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="assets/remark-zoom.js"></script>
<script src="https://platform.twitter.com/widgets.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9",
"navigation": {
"scroll": false
}
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
